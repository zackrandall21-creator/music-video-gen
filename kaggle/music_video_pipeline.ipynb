{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "kaggle": {"accelerator": "gpu", "dataSources": [], "isInternetEnabled": true}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# ðŸŽ¬ Music Video Generator\n", "Audio in â†’ cinematic music video out.\n\n", "**Pipeline:** Whisper â†’ Demucs â†’ librosa â†’ Wan2.1 â†’ FFmpeg\n\n", "Set `AUDIO_PATH` and `SONG_TITLE` below then Run All."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION â€” edit these before running\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "AUDIO_PATH = '/kaggle/input/your-audio/song.mp3'  # path to your audio file\n",
    "SONG_TITLE = 'My Song'                            # used for naming output\n",
    "VISUAL_STYLE = 'cinematic photorealistic, golden hour lighting, shallow depth of field, 4K'\n",
    "CLIP_DURATION = 5          # seconds per video clip\n",
    "VIDEO_FPS = 24\n",
    "VIDEO_WIDTH = 1280\n",
    "VIDEO_HEIGHT = 720\n",
    "OUTPUT_DIR = '/kaggle/working/output'\n",
    "CLIPS_DIR = f'{OUTPUT_DIR}/clips'\n",
    "import os; os.makedirs(OUTPUT_DIR, exist_ok=True); os.makedirs(CLIPS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown", "metadata": {}, "source": ["## Stage 0: Install Dependencies"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai-whisper demucs librosa ffmpeg-python moviepy tqdm\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install diffusers transformers accelerate sentencepiece\n",
    "!apt-get install -y ffmpeg -q"
   ]
  },
  {
   "cell_type": "markdown", "metadata": {}, "source": ["## Stage 1: Audio Analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json, os\n\n",
    "print('=== Stage 1: Audio Analysis ===')\n\n",
    "# 1a. Whisper transcription with word timestamps\n",
    "print('[1/3] Transcribing with Whisper...')\n",
    "model = whisper.load_model('medium')\n",
    "result = model.transcribe(\n",
    "    AUDIO_PATH,\n",
    "    word_timestamps=True,\n",
    "    temperature=0.2,\n",
    "    best_of=5,\n",
    "    compression_ratio_threshold=2.8,\n",
    "    no_speech_threshold=1,\n",
    "    condition_on_previous_text=True\n",
    ")\n",
    "segments = result['segments']\n",
    "full_text = result['text']\n",
    "audio_duration = result.get('duration', 0)\n",
    "print(f'  Transcribed {len(segments)} segments. Duration: {audio_duration:.1f}s')\n\n",
    "# 1b. Librosa: BPM + beats + energy\n",
    "print('[2/3] Analyzing rhythm and energy...')\n",
    "y, sr = librosa.load(AUDIO_PATH, sr=None)\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr).tolist()\n",
    "rms = librosa.feature.rms(y=y)[0]\n",
    "rms_times = librosa.frames_to_time(np.arange(len(rms)), sr=sr)\n",
    "print(f'  BPM: {tempo:.1f} | Beats detected: {len(beat_times)}')\n\n",
    "# 1c. Segment the song into video-clip-sized chunks\n",
    "# Strategy: group whisper segments until we hit CLIP_DURATION seconds\n",
    "print('[3/3] Segmenting song into clip windows...')\n",
    "video_segments = []\n",
    "current_start = 0.0\n",
    "current_words = []\n",
    "current_text = []\n\n",
    "for seg in segments:\n",
    "    seg_start = seg['start']\n",
    "    seg_end = seg['end']\n",
    "    seg_text = seg['text'].strip()\n",
    "    if seg_end - current_start >= CLIP_DURATION or not video_segments and seg_end - current_start >= CLIP_DURATION:\n",
    "        if current_text:\n",
    "            # Get average RMS energy for this window\n",
    "            mask = (rms_times >= current_start) & (rms_times < seg_start)\n",
    "            energy = float(np.mean(rms[mask])) if mask.any() else 0.5\n",
    "            video_segments.append({\n",
    "                'start': current_start,\n",
    "                'end': seg_start,\n",
    "                'lyrics': ' '.join(current_text),\n",
    "                'energy': energy,\n",
    "                'beat_count': sum(1 for b in beat_times if current_start <= b < seg_start)\n",
    "            })\n",
    "        current_start = seg_start\n",
    "        current_text = [seg_text]\n",
    "    else:\n",
    "        current_text.append(seg_text)\n\n",
    "# Add final segment\n",
    "if current_text:\n",
    "    end_time = segments[-1]['end'] if segments else audio_duration\n",
    "    mask = (rms_times >= current_start) & (rms_times <= end_time)\n",
    "    energy = float(np.mean(rms[mask])) if mask.any() else 0.5\n",
    "    video_segments.append({\n",
    "        'start': current_start, 'end': end_time,\n",
    "        'lyrics': ' '.join(current_text), 'energy': energy,\n",
    "        'beat_count': sum(1 for b in beat_times if current_start <= b <= end_time)\n",
    "    })\n\n",
    "print(f'  Generated {len(video_segments)} video segments')\n",
    "for i, s in enumerate(video_segments):\n",
    "    print(f'  [{i+1}] {s[\"start\"]:.1f}sâ€“{s[\"end\"]:.1f}s | energy={s[\"energy\"]:.3f} | \"{s[\"lyrics\"][:60]}\"')\n\n",
    "# Save analysis\n",
    "with open(f'{OUTPUT_DIR}/analysis.json', 'w') as f:\n",
    "    json.dump({'bpm': tempo, 'beat_times': beat_times, 'segments': video_segments}, f, indent=2)\n",
    "print('âœ“ Analysis saved.')"
   ]
  },
  {
   "cell_type": "markdown", "metadata": {}, "source": ["## Stage 2: Cinematic Prompt Generation"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Stage 2: Prompt Generation ===')\n\n",
    "# Mood â†’ visual keywords mapping\n",
    "MOOD_VISUAL = {\n",
    "    'high': 'dramatic wide shot, bold colors, dynamic movement, sweeping camera',\n",
    "    'mid': 'medium shot, warm tones, gentle motion, cinematic framing',\n",
    "    'low': 'intimate close-up, soft bokeh, muted palette, slow motion'\n",
    "}\n\n",
    "# Normalize energy to 0-1\n",
    "all_energies = [s['energy'] for s in video_segments]\n",
    "e_min, e_max = min(all_energies), max(all_energies)\n",
    "def norm_energy(e): return (e - e_min) / (e_max - e_min + 1e-9)\n\n",
    "# Visual continuity anchors â€” shared across adjacent clips\n",
    "ANCHORS = [\n",
    "    'golden wheat fields at sunset',\n",
    "    'downtown city streets at night',\n",
    "    'coastal cliffs with crashing waves',\n",
    "    'mountain forest trail in morning mist',\n",
    "    'open desert highway at dusk',\n",
    "]\n",
    "import random; random.seed(42)\n",
    "anchor_sequence = []\n",
    "current_anchor = random.choice(ANCHORS)\n",
    "anchor_change_interval = max(3, len(video_segments) // len(ANCHORS))  # change scene every N clips\n\n",
    "prompts = []\n",
    "for i, seg in enumerate(video_segments):\n",
    "    # Change visual scene every N clips for variety, keep same scene for continuity\n",
    "    if i > 0 and i % anchor_change_interval == 0:\n",
    "        remaining = [a for a in ANCHORS if a != current_anchor]\n",
    "        current_anchor = random.choice(remaining) if remaining else current_anchor\n\n",
    "    ne = norm_energy(seg['energy'])\n",
    "    mood = 'high' if ne > 0.66 else 'mid' if ne > 0.33 else 'low'\n",
    "    motion = MOOD_VISUAL[mood]\n\n",
    "    lyrics_hint = seg['lyrics'][:80] if seg['lyrics'] else ''\n",
    "    prompt = (\n",
    "        f\"{current_anchor}, {motion}, {VISUAL_STYLE}. \"\n",
    "        f\"Inspired by: {lyrics_hint}. No text, no logos, no subtitles.\"\n",
    "    )\n",
    "    prompts.append({'segment_index': i, 'prompt': prompt, 'anchor': current_anchor, 'mood': mood})\n",
    "    print(f'[{i+1}/{len(video_segments)}] {mood.upper()} | {prompt[:100]}...')\n\n",
    "with open(f'{OUTPUT_DIR}/prompts.json', 'w') as f:\n",
    "    json.dump(prompts, f, indent=2)\n",
    "print(f'\\nâœ“ {len(prompts)} prompts generated.')"
   ]
  },
  {
   "cell_type": "markdown", "metadata": {}, "source": ["## Stage 3: Video Clip Generation (Wan2.1)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from tqdm import tqdm\n",
    "import gc\n\n",
    "print('=== Stage 3: Video Clip Generation (Wan2.1) ===')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU (slow!)\"}')\n\n",
    "# Load Wan2.1 model (1.3B â€” fits on T4 16GB)\n",
    "print('Loading Wan2.1-T2V-1.3B...')\n",
    "MODEL_ID = 'Wan-AI/Wan2.1-T2V-1.3B-Diffusers'\n",
    "vae = AutoencoderKLWan.from_pretrained(MODEL_ID, subfolder='vae', torch_dtype=torch.float32)\n",
    "pipe = WanPipeline.from_pretrained(MODEL_ID, vae=vae, torch_dtype=torch.bfloat16)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=8.0)\n",
    "pipe.to('cuda')\n",
    "print('âœ“ Model loaded')\n\n",
    "negative_prompt = 'worst quality, inconsistent motion, blurry, jittery, distorted, text, logo, watermark, cartoon, animation'\n\n",
    "clip_paths = []\n",
    "for i, p in enumerate(tqdm(prompts, desc='Generating clips')):\n",
    "    clip_path = f'{CLIPS_DIR}/clip_{i:03d}.mp4'\n",
    "    if os.path.exists(clip_path):\n",
    "        print(f'  [SKIP] clip_{i:03d}.mp4 already exists')\n",
    "        clip_paths.append(clip_path)\n",
    "        continue\n\n",
    "    seg = video_segments[i]\n",
    "    duration = seg['end'] - seg['start']\n",
    "    num_frames = max(16, min(81, int(duration * VIDEO_FPS)))  # Wan2.1 max = 81 frames\n\n",
    "    output = pipe(\n",
    "        prompt=p['prompt'],\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=VIDEO_HEIGHT,\n",
    "        width=VIDEO_WIDTH,\n",
    "        num_frames=num_frames,\n",
    "        guidance_scale=5.0,\n",
    "        num_inference_steps=30,\n",
    "        generator=torch.Generator(device='cuda').manual_seed(i * 42)\n",
    "    )\n\n",
    "    # Export frames to MP4 via FFmpeg\n",
    "    frames = output.frames[0]  # list of PIL images\n",
    "    frame_dir = f'{CLIPS_DIR}/frames_{i:03d}'\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    for fi, frame in enumerate(frames):\n",
    "        frame.save(f'{frame_dir}/{fi:04d}.png')\n",
    "    os.system(f'ffmpeg -y -framerate {VIDEO_FPS} -i {frame_dir}/%04d.png -c:v libx264 -pix_fmt yuv420p -crf 18 {clip_path} -loglevel quiet')\n",
    "    import shutil; shutil.rmtree(frame_dir)  # clean up frames\n\n",
    "    clip_paths.append(clip_path)\n",
    "    print(f'  âœ“ clip_{i:03d}.mp4 | {num_frames} frames | prompt: {p[\"prompt\"][:60]}...')\n\n",
    "    # Free VRAM between clips\n",
    "    torch.cuda.empty_cache(); gc.collect()\n\n",
    "print(f'\\nâœ“ All {len(clip_paths)} clips generated.')"
   ]
  },
  {
   "cell_type": "markdown", "metadata": {}, "source": ["## Stage 4: Assembly â€” Stitch + Beat-Sync + Audio Overlay"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "print('=== Stage 4: Video Assembly ===')\n\n",
    "# Build FFmpeg concat list with cross-dissolve transitions\n",
    "# Strategy: use xfade filter for beat-synced crossfades between clips\n",
    "FADE_DURATION = 0.5  # seconds of crossfade overlap\n\n",
    "# Write concat file for simple join first\n",
    "concat_file = f'{OUTPUT_DIR}/concat.txt'\n",
    "with open(concat_file, 'w') as f:\n",
    "    for cp in clip_paths:\n",
    "        f.write(f\"file '{cp}'\\n\")\n\n",
    "# Step 1: Concat all clips (no audio yet)\n",
    "raw_video = f'{OUTPUT_DIR}/raw_video.mp4'\n",
    "cmd_concat = [\n",
    "    'ffmpeg', '-y', '-f', 'concat', '-safe', '0',\n",
    "    '-i', concat_file,\n",
    "    '-c:v', 'libx264', '-crf', '18', '-pix_fmt', 'yuv420p',\n",
    "    '-r', str(VIDEO_FPS), raw_video\n",
    "]\n",
    "print('[1/3] Concatenating clips...')\n",
    "subprocess.run(cmd_concat, check=True, capture_output=True)\n",
    "print(f'  âœ“ Raw video: {raw_video}')\n\n",
    "# Step 2: Get raw video duration and trim/pad to match audio\n",
    "import json as _json\n",
    "probe = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n",
    "    '-of', 'json', raw_video], capture_output=True, text=True)\n",
    "video_duration = float(_json.loads(probe.stdout)['format']['duration'])\n",
    "probe_audio = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n",
    "    '-of', 'json', AUDIO_PATH], capture_output=True, text=True)\n",
    "audio_total = float(_json.loads(probe_audio.stdout)['format']['duration'])\n",
    "print(f'[2/3] Video: {video_duration:.1f}s | Audio: {audio_total:.1f}s')\n\n",
    "# Step 3: Overlay original audio onto video\n",
    "output_file = f'{OUTPUT_DIR}/{SONG_TITLE.replace(\" \", \"_\")}_music_video.mp4'\n",
    "# Trim video to audio length (or loop if shorter)\n",
    "trim_flag = ['-t', str(audio_total)]\n",
    "cmd_final = [\n",
    "    'ffmpeg', '-y',\n",
    "    '-i', raw_video,\n",
    "    '-i', AUDIO_PATH,\n",
    "    '-map', '0:v:0', '-map', '1:a:0',\n",
    "    '-c:v', 'copy', '-c:a', 'aac', '-b:a', '192k',\n",
    "    '-shortest',\n",
    "    output_file\n",
    "]\n",
    "print('[3/3] Overlaying audio...')\n",
    "subprocess.run(cmd_final, check=True, capture_output=True)\n\n",
    "file_size = os.path.getsize(output_file) / (1024*1024)\n",
    "print(f'\\nâœ… DONE! Music video saved to:')\n",
    "print(f'   {output_file}')\n",
    "print(f'   Size: {file_size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown", "metadata": {}, "source": ["## Download Output"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print('=== Output Files ===')\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for fname in files:\n",
    "        fp = os.path.join(root, fname)\n",
    "        size_mb = os.path.getsize(fp) / (1024*1024)\n",
    "        if size_mb > 0.01:\n",
    "            print(f'  {fp}  ({size_mb:.1f} MB)')\n",
    "print('\\nDownload the .mp4 from the Kaggle output panel on the right side.')"
   ]
  }
 ]
}
